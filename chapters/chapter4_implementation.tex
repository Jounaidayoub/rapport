% Chapter 4: Implementation
\chapter{Implementation}

\section{Development Environment}

\subsection{Technical Stack}

The project was developed using the following technical stack:
\begin{itemize}
    \item \textbf{Programming Language}: Python 3.10+
    \item \textbf{Data Streaming}: Apache Kafka 3.x
    \item \textbf{Message Broker (Celery)}: Redis 7.x (also used for sliding window)
    \item \textbf{Task Queue}: Celery 5.x
    \item \textbf{Data Storage (Anomalies)}: Elasticsearch 8.x
    \item \textbf{Web Framework (API)}: FastAPI 0.100+
    \item \textbf{Containerization}: Docker \& Docker Compose
    \item \textbf{Operating System}: Linux (Ubuntu 22.04 LTS for development and deployment)
    \item \textbf{Python Libraries}:
    \begin{itemize}
        \item \texttt{kafka-python}: For Kafka producer and consumer clients.
        \item \texttt{redis}: For interacting with Redis.
        \item \texttt{elasticsearch}: The official Elasticsearch client for Python.
        \item \texttt{celery}: For defining and running asynchronous tasks.
        \item \texttt{fastapi}: For building the REST API.
        \item \texttt{uvicorn}: ASGI server for FastAPI.
        \item \texttt{pydantic}: For data validation in FastAPI.
        \item \texttt{numpy}: For numerical operations (mean, std dev).
        \item \texttt{pandas}: (Optional) For data manipulation if needed, primarily for report generation or data loading.
        \item \texttt{reportlab} / \texttt{WeasyPrint}: For PDF report generation.
        \item \texttt{python-json-logger}: For structured logging.
    \end{itemize}
\end{itemize}

\subsection{Development Tools}
\begin{itemize}
    \item \textbf{IDE}: Visual Studio Code with Python extension, Pylance, and Docker extension.
    \item \textbf{Version Control}: Git, with a repository hosted on GitHub/GitLab.
    \item \textbf{Debugging}: VS Code debugger, Celery Flower for monitoring tasks, Kibana for Elasticsearch inspection.
    \item \textbf{API Testing}: Postman / Insomnia, or curl.
    \item \textbf{Terminal}: Linux default terminal (bash/zsh).
\end{itemize}

\subsection{Dependency Management}

Python dependencies are managed using a \texttt{requirements.txt} file. This ensures that all developers and deployment environments use the same versions of libraries, promoting consistency and reducing compatibility issues. The full list of dependencies is available in the Appendix.

Dependencies are installed using: \texttt{pip install -r requirements.txt}

\subsection{Docker for Service Containerization}

Docker and Docker Compose are used to containerize each service (Kafka, Zookeeper, Elasticsearch, Redis, FastAPI application, Celery workers, Data Generator). This approach offers:
\begin{itemize}
    \item \textbf{Environment Consistency}: Ensures services run the same way regardless of the host machine.
    \item \textbf{Isolation}: Services run in isolated environments, preventing conflicts.
    \item \textbf{Simplified Deployment}: \texttt{docker-compose up} can start the entire multi-container application.
    \item \textbf{Scalability}: Docker Swarm or Kubernetes can be used to scale services in a production environment.
\end{itemize}
A \texttt{docker-compose.yml} file defines the services, networks, and volumes. (A snippet will be shown in Chapter 6).

\section{Key Module Implementation}

\subsection{Data Generation Module}

This module simulates stock price data for TSLA and publishes it to a Kafka topic.

\subsubsection{TSLA Data Structure and Simulation}
The data generator script creates JSON objects with \texttt{symbol}, \texttt{timestamp}, \texttt{price}, and \texttt{volume}.
It simulates price movements with a baseline price and random fluctuations. Volatility can be adjusted, and artificial anomalies (sudden spikes or drops) are injected periodically to test the detection logic.

\subsubsection{Kafka Producer Integration}
The script uses the \texttt{kafka-python} library to connect to the Kafka cluster and send messages. The full implementation is available in the appendix.

\textbf{Explanations:}
\begin{itemize}
    \item The \texttt{KafkaProducer} is configured with broker addresses and a JSON serializer.
    \item A function generates data points with random fluctuations and occasional anomalies.
    \item The main loop continuously generates and sends data to the \texttt{stock\_prices} topic.
    \item Error handling and graceful shutdown of the producer are included.
\end{itemize}

\textbf{Application of Factory Design Pattern for Data Generation:}
The data generation module can leverage the Factory design pattern to create different types of data generators (e.g., for different stock symbols, or different simulation models like normal, abnormal, or stress-test data). A `DataGeneratorFactory` could provide a method to return instances of `TSLADataGenerator`, `AAPLDataGenerator`, or `CustomAnomalyGenerator` based on configuration. This promotes modularity and extensibility, allowing new data generation strategies to be added without modifying existing client code.
\begin{figure}[h!]
    \centering
    \fbox{\parbox[c][5cm][c]{0.8\textwidth}{\centering Placeholder for Data Generator Factory Pattern Diagram}}
    \caption{Placeholder for Data Generator Factory Pattern Diagram}
    \label{fig:data_generator_factory_diagram}
\end{figure}

\subsection{Detection Module (Celery Task)}

This module consumes data from Kafka and performs Z-score based anomaly detection. It runs as a Celery task.

\subsubsection{Z-Score Algorithm Implementation}
The core logic involves fetching recent prices from Redis, calculating mean and standard deviation, and then the Z-score for the current price. The Celery task \texttt{detect\_anomaly} is triggered by messages from the \texttt{stock\_prices} Kafka topic. It uses Redis lists to maintain a sliding window of recent prices for each symbol. NumPy is used for efficient calculation of mean and standard deviation. If the Z-score exceeds the threshold, an anomaly event is created and published to another Kafka topic (\texttt{anomaly\_alerts}) and (conceptually) stored in Elasticsearch. Edge cases like insufficient data or zero standard deviation are handled. The full implementation is available in the appendix.

\textbf{Application of Factory Design Pattern for Anomaly Detection:}
The anomaly detection module can also benefit from the Factory design pattern to dynamically select and instantiate different detection algorithms (e.g., Z-score, Isolation Forest, ARIMA). An `AnomalyDetectorFactory` could provide a method to create instances of `ZScoreDetector`, `IsolationForestDetector`, or `ARIMADetector` based on configuration or runtime parameters. This allows for easy swapping or addition of new detection algorithms without altering the core processing logic.
\begin{figure}[h!]
    \centering
    \fbox{\parbox[c][5cm][c]{0.8\textwidth}{\centering Placeholder for Anomaly Detector Factory Pattern Diagram}}
    \caption{Placeholder for Anomaly Detector Factory Pattern Diagram}
    \label{fig:anomaly_detector_factory_diagram}
\end{figure}

\subsection{Storage Module (Elasticsearch Integration)}

This module is responsible for storing detected anomalies in Elasticsearch. Connection to Elasticsearch is typically managed using the official Python client. The full implementation for connection and anomaly storage is available in the appendix.

\subsubsection{Index Mapping}
The Elasticsearch index mapping (also detailed in Chapter 3) defines the structure and data types for anomaly documents. This is crucial for efficient searching and aggregation.

\subsubsection{Performance Optimizations}
\begin{itemize}
    \item \textbf{Bulk Indexing}: For high volumes, use Elasticsearch's bulk API to index multiple anomalies in a single request.
    \item \textbf{Connection Pooling}: Ensure the Elasticsearch client uses connection pooling.
    \item \textbf{Appropriate Sharding}: Configure the number of shards for the index based on expected data volume and query load.
    \item \textbf{Refresh Interval}: Adjust the index refresh interval if near real-time search is not strictly required for all use cases, to reduce indexing overhead.
\end{itemize}
 

\subsection{REST API (FastAPI)}

The FastAPI application provides endpoints for querying anomalies and generating reports.

\subsubsection{Implemented Endpoints}
Key endpoints include:
\begin{itemize}
    \item \texttt{GET /anomalies/}: Fetches anomalies, with query parameters for filtering by symbol, date range, etc.
    \item \texttt{GET /anomalies/\{anomaly\_id\}/}: Retrieves a specific anomaly by its ID.
    \texttt{POST /reports/generate/}: Initiates report generation (e.g., for a specific symbol and time range).
    \item \texttt{GET /reports/download/\{report\_id\}/}: Downloads a previously generated report.
\end{itemize}
The full implementation of these endpoints is available in the appendix.

\subsubsection{Parameter Validation}
FastAPI uses Pydantic models for automatic request and response validation, ensuring data integrity and providing clear error messages for invalid inputs.

\subsubsection{Error Handling}
Custom exception handlers and FastAPI's \texttt{HTTPException} are used to return appropriate HTTP status codes and error details to the client.

\section{Technical Challenges Encountered}

\subsection{Concurrency Management in Redis}
\begin{itemize}
    \item \textbf{Problem}: Multiple Celery workers accessing and updating Redis lists for the same symbol concurrently could lead to race conditions if not handled carefully (though Redis single-threaded operations on keys largely mitigate this for simple list operations like LPUSH/LTRIM).
    \item \textbf{Solution}: For more complex atomic operations, Redis transactions (MULTI/EXEC) or Lua scripting can be used. For this project, the atomicity of individual Redis commands (LPUSH, LTRIM, LRANGE) was generally sufficient for the sliding window logic. Distributed locks (e.g., using Redlock algorithm or a library) would be necessary if shared resources required more complex, multi-step atomic updates across different workers or services.
\end{itemize}

\subsection{Elasticsearch Query Performance}
\begin{itemize}
    \item \textbf{Problem}: As the volume of anomaly data grows, Elasticsearch queries for reports or API responses could become slow, especially with complex filters or aggregations.
    \item \textbf{Solution}:
        \begin{itemize}
            \item \textbf{Optimized Mappings}: Ensure fields used for filtering and sorting are indexed appropriately (e.g., \texttt{keyword} for exact matches, \texttt{date} for date ranges).
            \item \textbf{Targeted Queries}: Avoid \texttt{script} queries or overly broad \texttt{wildcard} queries where possible. Use \texttt{term} for exact matches and \texttt{range} for numerical/date fields.
            \item \textbf{Pagination}: Implement proper pagination for API endpoints returning large datasets.
            \item \textbf{Caching}: Consider caching frequent query results at the API layer or using Elasticsearch's query cache (though it has limitations).
            \item \textbf{Index Management}: Use time-based indices and potentially rollups for historical
